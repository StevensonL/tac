{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering de documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\elaur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/txt/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choisir une décennie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECADE = \"1900\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger tous les  fichiers de la décennie et en créer une liste de textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in sorted(os.listdir(data_path)) if f\"_{DECADE[:-1]}\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KB_JB838_1900-01-02_01-00002.txt',\n",
       " 'KB_JB838_1900-01-07_01-00007.txt',\n",
       " 'KB_JB838_1900-01-08_01-00003.txt',\n",
       " 'KB_JB838_1900-01-18_01-00008.txt',\n",
       " 'KB_JB838_1900-01-19_01-00004.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple de fichiers\n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [open(data_path + f, encoding=\"utf-8\").read() for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ LE SOIR Une importante manifestation franco-belge Le Comité franco-belge du Commerce, de l’Industrie et de l’Agriculture qui. au cours de l’année écoulée et dans les circonstances les plus difficiles a rendu aux relations économiques entre les deux pays des services considérables, donnera le 12 janvier, à 19 h. 30, un dîner à l’occasion de l’anniversaire de son installation. Ce comité, qui fonct'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple de textes\n",
    "texts[0][:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectoriser les documents à l'aide de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une fonction de pré-traitement\n",
    "def preprocessing(text, stem=True):\n",
    "    \"\"\" Tokenize text and remove punctuation \"\"\"\n",
    "    text = text.translate(string.punctuation)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instancier le modèle TF-IDF avec ses arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=preprocessing,\n",
    "    stop_words=stopwords.words('french'),\n",
    "    max_df=0.5,\n",
    "    min_df=0.1,\n",
    "    lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construire la matrice de vecteurs à l'aide de la fonction `fit_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tfidf_vectors \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(texts)\n",
      "File \u001b[1;32mc:\\Users\\elaur\\Desktop\\tac\\tac_venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2079\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2072\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2073\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2074\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2075\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2076\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2077\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2078\u001b[0m )\n\u001b[1;32m-> 2079\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2080\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2081\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2082\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\elaur\\Desktop\\tac\\tac_venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1330\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1332\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1333\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1334\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1335\u001b[0m             )\n\u001b[0;32m   1336\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1341\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\elaur\\Desktop\\tac\\tac_venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1220\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1216\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m   1217\u001b[0m         \u001b[39m# Ignore out-of-vocabulary items for fixed_vocab=True\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m-> 1220\u001b[0m j_indices\u001b[39m.\u001b[39;49mextend(feature_counter\u001b[39m.\u001b[39;49mkeys())\n\u001b[0;32m   1221\u001b[0m values\u001b[39m.\u001b[39mextend(feature_counter\u001b[39m.\u001b[39mvalues())\n\u001b[0;32m   1222\u001b[0m indptr\u001b[39m.\u001b[39mappend(\u001b[39mlen\u001b[39m(j_indices))\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tfidf_vectors = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Détail de la matrice\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tfidf_vectors\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_vectors' is not defined"
     ]
    }
   ],
   "source": [
    "# Détail de la matrice\n",
    "tfidf_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imprimer le vecteur tf-IDF du premier document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m pd\u001b[39m.\u001b[39mSeries(\n\u001b[1;32m----> 2\u001b[0m     tfidf_vectors[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtoarray()[\u001b[39m0\u001b[39m],\n\u001b[0;32m      3\u001b[0m     index\u001b[39m=\u001b[39mvectorizer\u001b[39m.\u001b[39mget_feature_names_out()\n\u001b[0;32m      4\u001b[0m     )\u001b[39m.\u001b[39msort_values(ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_vectors' is not defined"
     ]
    }
   ],
   "source": [
    "pd.Series(\n",
    "    tfidf_vectors[0].toarray()[0],\n",
    "    index=vectorizer.get_feature_names_out()\n",
    "    ).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprendre les vecteurs et leurs \"distances\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests sur nos documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_array = tfidf_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vecteur du document 0\n",
    "tfidf_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vecteur du document 1\n",
    "tfidf_array[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(tfidf_array[0], tfidf_array[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appliquer un algorithme de clustering sur les vecteurs TF-IDF des documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour en savoir plus sur le KMeans clustering :\n",
    "- https://medium.com/dataseries/k-means-clustering-explained-visually-in-5-minutes-b900cc69d175"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définir un nombre de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instancier le modèle K-Means et ses arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_model = KMeans(n_clusters=N_CLUSTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appliquer le clustering à l'aide de la fonction `fit_predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance intra et extra cluster, des centres tres eloignes l'un de l'autre et des points tres regroupe autour du centre\n",
    "clusters = km_model.fit_predict(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = collections.defaultdict(list)\n",
    "\n",
    "for idx, label in enumerate(clusters):\n",
    "    clustering[label].append(files[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dict(clustering))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser les clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réduire les vecteurs à 2 dimensions à l'aide de l'algorithme PCA\n",
    "Cette étape est nécessaire afin de visualiser les documents dans un espace 2D\n",
    "\n",
    "https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(tfidf_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduction des vecteurs a 10 dimension\n",
    "reduced_vectors[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Générer le plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = reduced_vectors[:, 0]\n",
    "y_axis = reduced_vectors[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(x_axis, y_axis, s=100, c=clusters)\n",
    "\n",
    "# Ajouter les centroïdes\n",
    "centroids = pca.transform(km_model.cluster_centers_)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],  marker = \"x\", s=100, linewidths = 2, color='black')\n",
    "\n",
    "# Ajouter la légende\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=set(clusters), title=\"Clusters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('tac_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "1371f28b61a57566aca84f831e458ad1b8215d81e6228e59fe3401f9a480bc22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
